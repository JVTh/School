{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JVTh/School/blob/main/Correct_lab3_nlp3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nils-holmberg/socs-qmd/blob/main/jnb/lab3_nlp3.ipynb)"
      ],
      "metadata": {
        "id": "CzbZy5WrZX8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text classification (naive bayes)"
      ],
      "metadata": {
        "id": "uTtmtKLTO4LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1G9-4_whd8BCObf3Gp6HDFFpQbDt_eTvy"
      ],
      "metadata": {
        "id": "rr9PPhGPQ-UL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f19a6a-238a-403e-828e-f3c9ca0aaae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G9-4_whd8BCObf3Gp6HDFFpQbDt_eTvy\n",
            "To: /content/education24.xlsx\n",
            "\r  0% 0.00/302k [00:00<?, ?B/s]\r100% 302k/302k [00:00<00:00, 76.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fUXZtNs8RKMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = \"education24.xlsx\"\n",
        "df = pd.read_excel(fp, header=None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OPaAo31bRMZi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4db38e56-56c0-4423-d402-ce19ddb62af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0                                                  1\n",
              "0  0  Have you spoken with the teacher to clarify th...\n",
              "1  1  Students are supposed to watch the assigned vi...\n",
              "2  2  So, let me get this straight, you_x0019_re pre...\n",
              "3  3   I HIGHLY doubt that happened. It could have b...\n",
              "4  4  Bad. Faster students slowed down so slow stude..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb1f1023-6831-4e97-bda9-a152ad9da580\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Have you spoken with the teacher to clarify th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Students are supposed to watch the assigned vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>So, let me get this straight, you_x0019_re pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I HIGHLY doubt that happened. It could have b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Bad. Faster students slowed down so slow stude...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb1f1023-6831-4e97-bda9-a152ad9da580')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb1f1023-6831-4e97-bda9-a152ad9da580 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb1f1023-6831-4e97-bda9-a152ad9da580');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23b8c880-bd26-458f-894c-3ede2349a932\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23b8c880-bd26-458f-894c-3ede2349a932')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23b8c880-bd26-458f-894c-3ede2349a932 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "riqSiWaxRTAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8788a497-472b-43ec-abad-08d0cd0463b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define features and outcomes\n"
      ],
      "metadata": {
        "id": "06ICJ4xscxn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer instance with NLTK stopwords\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words=nltk_stopwords)\n",
        "\n",
        "# Select the first two rows of the \"text\" column from the df dataframe\n",
        "documents = df['text'].iloc[:3]\n",
        "\n",
        "# Fit and transform the documents using CountVectorizer\n",
        "dtm = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the DTM to a Pandas DataFrame\n",
        "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Use the actual row indices as document IDs\n",
        "dtm_df.index = documents.index\n",
        "\n",
        "# Display the resulting Document-Term Matrix (DTM)\n",
        "dtm_df\n"
      ],
      "metadata": {
        "id": "Q0qeIA2WckjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "38ff542c-e896-48bc-f46c-f61252311dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-19c8735bddc0>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Select the first two rows of the \"text\" column from the df dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Fit and transform the documents using CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TF-IDF Vectorizer instance with NLTK stopwords\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=nltk_stopwords)\n",
        "\n",
        "# Select the first two rows of the \"text\" column from the df dataframe\n",
        "documents = df['text'].iloc[:3]\n",
        "\n",
        "# Fit and transform the documents using TF-IDF Vectorizer\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the TF-IDF matrix to a Pandas DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Use the actual row indices as document IDs\n",
        "tfidf_df.index = documents.index\n",
        "\n",
        "# Display the resulting TF-IDF matrix\n",
        "tfidf_df\n"
      ],
      "metadata": {
        "id": "CVzPsp8Aczon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split training and test data"
      ],
      "metadata": {
        "id": "NrCcgQQLRlvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Assuming your Excel columns are named 'text' and 'sentiment'\n",
        "X = df['text']  # Text data\n",
        "y = df['sentiment']  # Binary sentiment labels\n",
        "\n",
        "# Step 2: Perform TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features as needed\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the dataset into training (750 rows) and testing (250 rows)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Now, X_train and y_train contain the training data and labels (750 rows),\n",
        "# and X_test and y_test contain the testing data and labels (250 rows).\n"
      ],
      "metadata": {
        "id": "HVh7N3gwQ30t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train naive bayes classifier\n",
        "\n",
        "## validate model performance"
      ],
      "metadata": {
        "id": "k_w_RmKJRwwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Step 4: Train a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "\n",
        "# Step 7: Create and plot a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "11kEuWuwQ5Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save classification model\n",
        "\n",
        "In a binary classification task, where you are trying to classify data into one of two classes (typically referred to as the positive class and the negative class), several performance measures can help you assess the quality of your model. Here are the commonly used performance measures: Accuracy, Precision, Recall, and F1-score:\n",
        "\n",
        "1. **Accuracy:**\n",
        "   - Accuracy is a straightforward measure that calculates the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances in your dataset. It provides an overall assessment of how well your model is performing.\n",
        "   - Formula: `(TP + TN) / (TP + TN + FP + FN)`\n",
        "   - TP: True Positives (correctly predicted positive instances)\n",
        "   - TN: True Negatives (correctly predicted negative instances)\n",
        "   - FP: False Positives (negative instances incorrectly predicted as positive)\n",
        "   - FN: False Negatives (positive instances incorrectly predicted as negative)\n",
        "\n",
        "   High accuracy is desirable, but it can be misleading, especially in imbalanced datasets, where one class significantly outnumbers the other.\n",
        "\n",
        "2. **Precision:**\n",
        "   - Precision measures the accuracy of positive predictions made by the model. It calculates the ratio of true positives to the total number of instances predicted as positive (true positives plus false positives).\n",
        "   - Formula: `TP / (TP + FP)`\n",
        "   - Precision focuses on minimizing false positives. It's valuable when the cost of false positives is high, such as in medical diagnoses.\n",
        "\n",
        "3. **Recall (Sensitivity or True Positive Rate):**\n",
        "   - Recall measures the model's ability to capture all positive instances in the dataset. It calculates the ratio of true positives to the total number of actual positive instances (true positives plus false negatives).\n",
        "   - Formula: `TP / (TP + FN)`\n",
        "   - Recall is crucial when you want to avoid missing positive cases. For example, in disease detection, it's essential to have high recall to minimize false negatives.\n",
        "\n",
        "4. **F1-Score:**\n",
        "   - The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall, which can be helpful when you want to strike a balance between minimizing false positives and false negatives.\n",
        "   - Formula: `2 * (Precision * Recall) / (Precision + Recall)`\n",
        "   - The F1-score ranges from 0 to 1, where a higher score indicates a better balance between precision and recall.\n",
        "\n",
        "In summary:\n",
        "- **Accuracy** provides an overall view of your model's performance but may not be suitable for imbalanced datasets.\n",
        "- **Precision** is useful when minimizing false positives is a priority.\n",
        "- **Recall** is valuable when minimizing false negatives is critical.\n",
        "- **F1-Score** balances precision and recall, making it a suitable measure when you want to find a compromise between the two.\n",
        "\n",
        "The choice of which metric(s) to prioritize depends on the specific goals and requirements of your binary classification task. It's often a good practice to consider a combination of these metrics and evaluate your model's performance comprehensively.\n",
        "\n",
        "## make inference on new samples"
      ],
      "metadata": {
        "id": "_iyaZL5MSRVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model and TF-IDF vectorizer to files\n",
        "model_filename = 'sentiment_model.pkl'\n",
        "vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
        "\n",
        "joblib.dump(clf, model_filename)\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_filename)\n"
      ],
      "metadata": {
        "id": "mVybW_afSO1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the model and vectorizer for inference:\n",
        "loaded_model = joblib.load(model_filename)\n",
        "loaded_vectorizer = joblib.load(vectorizer_filename)\n",
        "\n",
        "# Example: Using the loaded model and vectorizer to predict sentiment for a new sample string\n",
        "new_sample = \"This phone is not bad\"\n",
        "# Vectorize the new sample using the loaded vectorizer\n",
        "new_sample_tfidf = loaded_vectorizer.transform([new_sample])\n",
        "# Use the loaded model for prediction\n",
        "prediction = loaded_model.predict(new_sample_tfidf)\n",
        "\n",
        "# Convert the prediction to a human-readable label if needed\n",
        "sentiment_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "print(f\"Predicted sentiment: {sentiment_label}\")\n"
      ],
      "metadata": {
        "id": "FP5-WeyGUKjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse transform the TF-IDF vectors to human-readable text\n",
        "reversed_texts = loaded_vectorizer.inverse_transform(X_test)\n",
        "\n",
        "# Create a DataFrame for the testing dataset with original and predicted sentiments\n",
        "test_df = pd.DataFrame({'Original Text': [' '.join(text) for text in reversed_texts],\n",
        "                        'Original Sentiment': y_test,\n",
        "                        'Predicted Sentiment': y_pred})\n",
        "\n",
        "# Access the original unaltered natural language text from the 'df' dataframe using row indices\n",
        "original_texts = df.iloc[y_test.index]['text'].tolist()\n",
        "\n",
        "# Add the original texts as the first column in 'test_df'\n",
        "test_df.insert(0, 'Original Unaltered Text', original_texts)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "test_df\n"
      ],
      "metadata": {
        "id": "YK_hwf4IbdV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentiment analysis (transformers)"
      ],
      "metadata": {
        "id": "OPaTPQi2ePtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n"
      ],
      "metadata": {
        "id": "NrulusuBesGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Use the pipeline to predict sentiment on the original unaltered texts\n",
        "test_df['HuggingFace Prediction'] = test_df['Original Unaltered Text'].apply(lambda text: nlp(text)[0])\n",
        "\n",
        "# Extract the predicted sentiment labels from the pipeline results and convert to 0 (negative) or 1 (positive)\n",
        "test_df['HuggingFace Confidence'] = test_df['HuggingFace Prediction'].apply(lambda prediction: prediction['score'])\n",
        "test_df['HuggingFace Prediction'] = test_df['HuggingFace Prediction'].apply(lambda prediction: 0 if prediction['label'] == 'NEGATIVE' else 1)\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(test_df['Original Sentiment'], test_df['HuggingFace Prediction'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Sentiment\")\n",
        "plt.ylabel(\"Original Sentiment\")\n",
        "plt.title(\"Confusion Matrix (Hugging Face Transformers)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ORTVNh6xevW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "ziWHs0p_lY4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(\"This phone is not good\")"
      ],
      "metadata": {
        "id": "gwRdx-sCiAJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix for Multinomial Naive Bayes predictions\n",
        "conf_matrix_nb = confusion_matrix(test_df['Original Sentiment'], test_df['Predicted Sentiment'])\n",
        "\n",
        "# Calculate the confusion matrix for Hugging Face Transformers predictions\n",
        "conf_matrix_hf = confusion_matrix(test_df['Original Sentiment'], test_df['HuggingFace Prediction'])\n",
        "\n",
        "# Create subplots to display confusion matrices side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "\n",
        "# Plot the Multinomial Naive Bayes confusion matrix\n",
        "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[0])\n",
        "axes[0].set_title(\"Multinomial Naive Bayes\")\n",
        "axes[0].set_xlabel(\"Predicted Sentiment\")\n",
        "axes[0].set_ylabel(\"Original Sentiment\")\n",
        "\n",
        "# Plot the Hugging Face Transformers confusion matrix\n",
        "sns.heatmap(conf_matrix_hf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[1])\n",
        "axes[1].set_title(\"Hugging Face Transformers\")\n",
        "axes[1].set_xlabel(\"Predicted Sentiment\")\n",
        "axes[1].set_ylabel(\"Original Sentiment\")\n",
        "\n",
        "# Display the figure\n",
        "plt.show()\n",
        "\n",
        "# Save the figure to a PNG file\n",
        "plt.savefig('confusion_matrices.png')\n",
        "plt.close()\n",
        "\n",
        "# Save the 'test_df' DataFrame to a CSV file\n",
        "test_df.to_csv('test_df.tsv', sep=\"\\t\", index=False)\n"
      ],
      "metadata": {
        "id": "eT1gQGJBiOIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plug in your own data"
      ],
      "metadata": {
        "id": "bqO8Cz2VMF_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1EMzJxxoBaN_NbvF7xhoc09K82vQ6H_LX"
      ],
      "metadata": {
        "id": "iXZ4Uk7UySiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = \"content.xlsx\"\n",
        "df = pd.read_excel(fp, header=None, names=['id', 'text'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pta9V16WKmgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pipeline to predict sentiment on the original unaltered texts\n",
        "df['hf_nlp'] = df['text'].apply(lambda text: nlp(text)[0])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SMRvzCvTI5sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the predicted sentiment labels from the pipeline results and convert to 0 (negative) or 1 (positive)\n",
        "df['hf_prediction'] = df['hf_nlp'].apply(lambda prediction: 0 if prediction['label'] == 'NEGATIVE' else 1)\n",
        "df['hf_confidence'] = df['hf_nlp'].apply(lambda prediction: prediction['score'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vrYeMxD7JWY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hf_prediction.mean()"
      ],
      "metadata": {
        "id": "oQ5u3O75Lr6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## swedish language texts"
      ],
      "metadata": {
        "id": "16ItX1Way3gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KBLab/megatron-bert-large-swedish-cased-165k\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"KBLab/robust-swedish-sentiment-multiclass\")\n",
        "\n",
        "# Example text\n",
        "swedish_text = \"Jag älskar denna produkt!\"  # \"I love this product!\"\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "result = classifier(swedish_text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "6cIMf5xCqja3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swedish_text = \"denna produkt är mycket bra\"  # \"I love this product!\"\n",
        "result = classifier(swedish_text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-5Gc-NCMrYi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spacy language models"
      ],
      "metadata": {
        "id": "gu7tWB2wO31f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# medium size language model with word vectors\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "id": "CESDGvnaVqk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvSRg1InOtwF"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Define random words\n",
        "word = \"cat\"\n",
        "\n",
        "# Process the words to get their word vectors\n",
        "token = nlp(word)\n",
        "\n",
        "# Check if the token has a vector\n",
        "if token.has_vector:\n",
        "    # Print the word vector as a NumPy array\n",
        "    print(f\"Vector for '{word}':\")\n",
        "    print(token.vector)\n",
        "else:\n",
        "    print(f\"No vector available for '{word}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(token.vector))"
      ],
      "metadata": {
        "id": "XBpkqRDAWn7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two random words\n",
        "word1 = \"cat\"\n",
        "word2 = \"tiger\"\n",
        "\n",
        "# Process the words to get their word vectors\n",
        "token1 = nlp(word1)\n",
        "token2 = nlp(word2)\n",
        "\n",
        "# Check if the tokens are valid words (have word vectors)\n",
        "if token1.has_vector and token2.has_vector:\n",
        "    # Compute the cosine similarity between the word vectors\n",
        "    similarity = token1.similarity(token2)\n",
        "    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "else:\n",
        "    print(\"One or both of the words do not have word vectors available in the spaCy model.\")\n"
      ],
      "metadata": {
        "id": "hQvp1WdRWUQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spacy text inference"
      ],
      "metadata": {
        "id": "0KQCSQhCo3D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp = \"content.xlsx\"\n",
        "df = pd.read_excel(fp, header=None, names=['id', 'image', 'text'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "f0M5ffTaU7Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model for sentence segmentation\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to split text into sentences and create a new DataFrame\n",
        "def split_sentences(df):\n",
        "    sentence_data = {'id': [], 'sentence_number': [], 'sentence_text': []}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        doc = nlp(row['text'])\n",
        "        for i, sentence in enumerate(doc.sents, start=1):\n",
        "            sentence_data['id'].append(row['id'])\n",
        "            sentence_data['sentence_number'].append(i)\n",
        "            sentence_data['sentence_text'].append(sentence.text)\n",
        "\n",
        "    return pd.DataFrame(sentence_data)\n",
        "\n",
        "# Create a new DataFrame with sentences\n",
        "sentences_df = split_sentences(df)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(sentences_df)\n"
      ],
      "metadata": {
        "id": "_SsSlsK8o8Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process sentences with spaCy and store results in a list of dictionaries\n",
        "def process_sentences_with_spacy(df):\n",
        "    processed_sentences = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text_id = row['id']\n",
        "        sentence_text = row['sentence_text']\n",
        "\n",
        "        doc = nlp(sentence_text)\n",
        "\n",
        "        # Process each token in the sentence\n",
        "        processed_tokens = []\n",
        "        for token in doc:\n",
        "            processed_token = {\n",
        "                'text': token.text,\n",
        "                'lemma': token.lemma_,\n",
        "                'entity': token.ent_type_,\n",
        "                'pos': token.pos_\n",
        "            }\n",
        "            processed_tokens.append(processed_token)\n",
        "\n",
        "        processed_sentence = {\n",
        "            'id': text_id,\n",
        "            'sentence_text': sentence_text,\n",
        "            'tokens': processed_tokens\n",
        "        }\n",
        "        processed_sentences.append(processed_sentence)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Apply spaCy processing and create a new column 'spacy_nlp'\n",
        "sentences_df['spacy_nlp'] = process_sentences_with_spacy(sentences_df)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(sentences_df)\n"
      ],
      "metadata": {
        "id": "UkaeqKWHpj34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze sentences with spaCy and store results in a list of dictionaries\n",
        "def analyze_sentences_with_spacy(df):\n",
        "    analyzed_sentences = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        doc = nlp(row['sentence_text'])\n",
        "\n",
        "        # Process each token in the sentence\n",
        "        analyzed_tokens = []\n",
        "        for token in doc:\n",
        "            analyzed_token = {\n",
        "                'text': token.text,\n",
        "                'lemma': token.lemma_,\n",
        "                'entity': token.ent_type_,\n",
        "                'pos': token.pos_\n",
        "            }\n",
        "            analyzed_tokens.append(analyzed_token)\n",
        "\n",
        "        analyzed_sentence = {\n",
        "            'id': row['id'],\n",
        "            'sentence_number': row['sentence_number'],\n",
        "            'sentence_text': row['sentence_text'],\n",
        "            'spacy_nlp': analyzed_tokens\n",
        "        }\n",
        "        analyzed_sentences.append(analyzed_sentence)\n",
        "\n",
        "    return analyzed_sentences\n",
        "\n",
        "# Analyze sentences with spaCy and create a new column 'spacy_nlp'\n",
        "sentences_df['spacy_nlp'] = analyze_sentences_with_spacy(sentences_df)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "sentences_df"
      ],
      "metadata": {
        "id": "qq7_xM5_r32U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze sentences by token, lemma, entity, and pos\n",
        "def analyze_sentence_with_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        token_info = {\n",
        "            'token': token.text,\n",
        "            'lemma': token.lemma_,\n",
        "            'entity': token.ent_type_,\n",
        "            'pos': token.pos_\n",
        "        }\n",
        "        tokens.append(token_info)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Create a new DataFrame 'tokens_df'\n",
        "tokens_data = []\n",
        "\n",
        "for _, row in sentences_df.iterrows():\n",
        "    id_val = row['id']\n",
        "    sentence_number = row['sentence_number']\n",
        "    sentence_text = row['sentence_text']\n",
        "\n",
        "    tokens_info = analyze_sentence_with_spacy(sentence_text)\n",
        "\n",
        "    for token_info in tokens_info:\n",
        "        tokens_data.append({\n",
        "            'id': id_val,\n",
        "            'sentence_number': sentence_number,\n",
        "            'token': token_info['token'],\n",
        "            'lemma': token_info['lemma'],\n",
        "            'entity': token_info['entity'],\n",
        "            'pos': token_info['pos']\n",
        "        })\n",
        "\n",
        "tokens_df = pd.DataFrame(tokens_data)\n",
        "\n",
        "# Print the resulting 'tokens_df'\n",
        "print(tokens_df)"
      ],
      "metadata": {
        "id": "VcrJY0YLxUNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with 'None' value in the 'entity' column\n",
        "filtered_tokens_df = tokens_df[tokens_df['entity'].notna()]\n",
        "filtered_tokens_df = tokens_df[tokens_df['entity'] != '']\n",
        "\n",
        "# Sort the values in the 'entity' column\n",
        "#filtered_tokens_df['entity'] = filtered_tokens_df['entity'].astype(str)\n",
        "#filtered_tokens_df = filtered_tokens_df.sort_values(by='entity')\n",
        "filtered_tokens_df = filtered_tokens_df.loc[filtered_tokens_df['entity'].astype(str).sort_values().index]\n",
        "\n",
        "# Create a frequency table\n",
        "entity_frequency = filtered_tokens_df['entity'].value_counts().reset_index()\n",
        "entity_frequency.columns = ['entity', 'frequency']\n",
        "\n",
        "# Create a frequency diagram of unique values in the 'entity' column\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style=\"darkgrid\")\n",
        "entity_plot = sns.countplot(x=\"entity\", data=filtered_tokens_df, palette=\"Set3\")\n",
        "entity_plot.set_title(\"Entity Frequency Diagram\")\n",
        "entity_plot.set_xlabel(\"Entity\")\n",
        "entity_plot.set_ylabel(\"Frequency\")\n",
        "\n",
        "# Rotate x-axis labels for better readability (optional)\n",
        "entity_plot.set_xticklabels(entity_plot.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LmNusDnxzAQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "    CARDINAL - Numerals that do not fall under another type\n",
        "    DATE - Absolute or relative dates or periods\n",
        "    EVENT - Named hurricanes, battles, wars, sports events, etc.\n",
        "    FAC - Buildings, airports, highways, bridges, etc.\n",
        "    GPE - Countries, cities, states\n",
        "    LANGUAGE - Any named language\n",
        "    LAW - Named documents made into laws.\n",
        "    LOC - Non-GPE locations, mountain ranges, bodies of water\n",
        "    MONEY - Monetary values, including unit\n",
        "    NORP - Nationalities or religious or political groups\n",
        "    ORDINAL - \"first\", \"second\", etc.\n",
        "    PERCENT - Percentage, including \"%\"\n",
        "    PERSON - People, including fictional\n",
        "    PRODUCT - Objects, vehicles, foods, etc. (not services)\n",
        "    QUANTITY - Measurements, as of weight or distance\n",
        "    TIME - Times smaller than a day\n",
        "    WORK_OF_ART - Titles of books, songs, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "4yuWhhJx7G_K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTEYISww1eXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}